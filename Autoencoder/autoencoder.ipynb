{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano as th\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    \n",
    "    def __init__(self, layers, activ='tanh', update='sgd', lr=0.0003, batch=1, memo = 0.3):\n",
    "        self.x = T.matrix()\n",
    "        self.y_hat = T.matrix()\n",
    "        self.layers = layers\n",
    "        self.activ, self.update = self.choose(activ, update)\n",
    "        self.batch = batch; self.lr = lr; self.memo = memo\n",
    "        self.weights = []; self.biases = []\n",
    "        self.auxiliary = []\n",
    "        self.a_n = [self.x]\n",
    "        \n",
    "    def choose(self, activ, update):\n",
    "        \"\"\"Choose the activation and update function\"\"\"\n",
    "        Acti = dict({'tanh':self.tanh,'sigmoid':self.sigmoid,'ReLU':self.ReLU,'linear':self.linear})\n",
    "        Upda = dict({'sgd':self.sgd,'NAG':self.NAG,'RMSProp':self.RMSProp,'momentum':self.momentum})\n",
    "        return Acti[activ], Upda[update] \n",
    "        \n",
    "    def architecture(self, cons, code_layer):\n",
    "        \"\"\"Build up the architecture by theano\"\"\"\n",
    "        for i in range(len(self.layers)-1):\n",
    "            #Initialize shared variables\n",
    "            self.weights.append(th.shared(cons*np.random.randn(self.layers[i],self.layers[i+1])))\n",
    "            self.biases.append(th.shared(cons*np.random.randn(self.layers[i+1])))\n",
    "            #Building architecture\n",
    "            a_next = self.activ(T.dot(self.a_n[i],self.weights[i]) + self.biases[i].dimshuffle('x',0))\n",
    "            self.a_n.append(a_next)\n",
    "        \n",
    "        #help the optimization\n",
    "        for param in (self.weights+self.biases):    \n",
    "            self.auxiliary.append(th.shared(np.zeros(param.get_value().shape)))\n",
    "            \n",
    "        self.encode = th.function([self.x],self.a_n[code_layer]) \n",
    "        self.decode = th.function([self.a_n[code_layer]],self.a_n[-1])\n",
    "        \n",
    "        #Calculate the cost and gradients\n",
    "        Cost = (T.sum((self.a_n[-1]-self.y_hat)**2))/self.batch\n",
    "        grads = T.grad(Cost,self.weights+self.biases,disconnected_inputs='ignore') \n",
    "        #Update parameters\n",
    "        self.gradient_2 = th.function(inputs=[self.x,self.y_hat],updates=\n",
    "                                      self.update(self.weights+self.biases,grads,self.auxiliary),outputs=Cost)\n",
    "            \n",
    "    def fit(self, X, code_layer=1, epoch=10, print_every=1, cons=0.3):\n",
    "        \"\"\"fitting the data (unsupervised learning)\"\"\"\n",
    "        self.architecture(cons, code_layer)\n",
    "        start = time.clock(); self.Cost_Record = []   \n",
    "        for j in range(epoch):\n",
    "            costs = 0\n",
    "            rounds = int(X.shape[0]/self.batch)\n",
    "            X_permuted = X[np.random.permutation(X.shape[0])]\n",
    "            \n",
    "            for i in range(rounds):\n",
    "                batch_X = X_permuted[i*self.batch:(i+1)*self.batch]\n",
    "                costs += self.gradient_2(batch_X,batch_X)\n",
    "\n",
    "            self.Cost_Record.append(costs/rounds)\n",
    "            \n",
    "            if j % print_every==0:\n",
    "                print(\"Epoch %d ; Cost: %f; %f seconds used.\"%(j+1,self.Cost_Record[-1],(time.clock()-start)))\n",
    "    \n",
    "    def encode(self, X):\n",
    "        return self.encode(X)\n",
    "    \n",
    "    def decode(self, X):\n",
    "        return self.decode(X)\n",
    "    \n",
    "    ##### Optimization methods #####\n",
    "    def sgd(self,para,grad,_):\n",
    "        \"\"\"optimized by gradient descent\"\"\"\n",
    "        return [(para[ix], para[ix]-self.lr*grad[ix]) for ix in range(len(grad))]\n",
    "\n",
    "    def NAG(self,para,grad,Real):\n",
    "        \"\"\"optimized by Nesterov accelerated gadient(NAG)\"\"\"\n",
    "        updates = []\n",
    "        for ix in range(len(grad)):\n",
    "            #grad[ix] = T.clip(grad[ix],-1,1)\n",
    "            gradient = -(self.lr/self.batch)*grad[ix]\n",
    "            spy_position = (1+self.memo)*(para[ix]+gradient)-self.memo*Real[ix]\n",
    "            updates.append((para[ix], spy_position))\n",
    "            updates.append((Real[ix], para[ix]+gradient))\n",
    "        return updates\n",
    "    \n",
    "    def momentum(self,para,grad,Momentum):\n",
    "        \"\"\"optimized by momentum\"\"\"\n",
    "        updates = []\n",
    "        for ix in range(len(grad)):\n",
    "            #grad[ix] = T.clip(grad[ix],-1,1)\n",
    "            direction = (self.memo)*Momentum[ix] - (self.lr/self.batch)*grad[ix]\n",
    "            updates.append((para[ix], para[ix]+direction))\n",
    "            updates.append((Momentum[ix], direction))\n",
    "        return updates\n",
    "    \n",
    "    def RMSProp(self,para,grad,Sigma_square):\n",
    "        \"\"\"optimized by RMSProp\"\"\"\n",
    "        updates = []; alpha = self.memo\n",
    "        for ix in range(len(grad)):\n",
    "            #grad[ix] = T.clip(grad[ix],-1,1)\n",
    "            gradient = grad[ix]/self.batch\n",
    "            Factor = Sigma_square[ix]*alpha+(1-alpha)*(gradient**2)\n",
    "            direction = -(self.lr)*gradient/(T.sqrt(Factor)+0.001)\n",
    "            updates.append((para[ix], para[ix]+direction))\n",
    "            updates.append((Sigma_square[ix], Factor))\n",
    "        return updates\n",
    "    \n",
    "    ##### Activation functions #####               \n",
    "    def tanh(self, Z):\n",
    "        return T.tanh(Z)\n",
    "    \n",
    "    def ReLU(self, Z):\n",
    "        return T.switch(Z<0,0,Z)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1/(1+T.exp(-Z))\n",
    "    \n",
    "    def linear(self, Z):\n",
    "        return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Cost: 3.732154; 0.007659 seconds used.\n",
      "Epoch 2 ; Cost: 2.896641; 0.014883 seconds used.\n",
      "Epoch 3 ; Cost: 2.757835; 0.022593 seconds used.\n",
      "Epoch 4 ; Cost: 2.831620; 0.029912 seconds used.\n",
      "Epoch 5 ; Cost: 2.593290; 0.037199 seconds used.\n",
      "Epoch 6 ; Cost: 2.674831; 0.044469 seconds used.\n",
      "Epoch 7 ; Cost: 2.499200; 0.051758 seconds used.\n",
      "Epoch 8 ; Cost: 2.543282; 0.059048 seconds used.\n",
      "Epoch 9 ; Cost: 2.458749; 0.067336 seconds used.\n",
      "Epoch 10 ; Cost: 2.437635; 0.075334 seconds used.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Data/train.dat',header=None,delim_whitespace=True)\n",
    "coder = Autoencoder([9,64,32,16,2,16,32,64,9], batch=4, activ='tanh', update='RMSProp', memo=0.9, lr=0.003)\n",
    "coder.fit(data.values, code_layer=4, epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode:\n",
      " [[ 0.9022 -0.2044]\n",
      " [-0.8444 -0.595 ]\n",
      " [-0.0083 -0.9609]\n",
      " [-0.7901 -0.2062]\n",
      " [-0.083  -0.2733]]\n",
      "decode:\n",
      " [[ 0.4606 -0.1849 -0.1809  0.0117 -0.7336 -0.1489  0.1116 -0.4908 -0.5451]\n",
      " [-0.5626 -0.0559  0.312  -0.6131  0.1229 -0.227  -0.7466  0.5354 -0.0484]\n",
      " [ 0.1846  0.4433  0.5595 -0.295  -0.368   0.1861 -0.7999  0.2592 -0.051 ]\n",
      " [-0.5403 -0.0397 -0.0683 -0.5874  0.1923 -0.3252 -0.6156  0.4338 -0.1083]\n",
      " [ 0.163   0.3571  0.2002 -0.4426 -0.1815 -0.0698 -0.4926  0.2595  0.1824]]\n",
      "original:\n",
      " [[ 0.8105 -0.35    0.4769  0.4541 -0.9829  0.5252  0.3838 -0.3408 -0.4824]\n",
      " [-0.6273 -0.2097  0.9404  0.1143  0.3487 -0.5206  0.0061  0.5024 -0.6687]\n",
      " [ 0.1624 -0.1173  0.426  -0.3607 -0.6632  0.4431 -0.8355  0.7206 -0.8977]\n",
      " [-1.      0.7758 -0.267  -0.888  -0.1099 -0.9183 -0.4086  0.8962  0.5841]\n",
      " [ 0.8464  0.1762  0.2729  0.2724  0.8155  0.6096 -0.2844  0.98    0.3302]]\n"
     ]
    }
   ],
   "source": [
    "code = coder.encode(data.values)\n",
    "np.set_printoptions(4)\n",
    "print('encode:\\n',code[:5])\n",
    "print('decode:\\n',coder.decode(code)[:5])\n",
    "print('original:\\n',data.values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
